# Chapter 1: Introduction to Privacy for the IT Professional

This chapter establishes the foundational concepts of privacy for information technology professionals, distinguishing between security and privacy, outlining key privacy definitions, and introducing frameworks for managing data throughout its life cycle.

### 1.2 What Is Privacy?
The text describes privacy as "pluralistic," meaning it encompasses multiple definitions and dimensions rather than a single concept. The chapter highlights four prominent viewpoints used to define privacy:
* **Alan Westin’s Four States of Privacy:**
    * *Solitude:* Separation from the group; freedom from observation.
    * *Intimacy:* Acting as part of a small unit; negotiating secrecy rules.
    * *Anonymity:* Freedom from identification and surveillance in public.
    * *Reserve:* Psychological barrier against unwanted intrusion; the ability to withhold communication.
* **Helen Nissenbaum’s Contextual Integrity:** Privacy is defined by norms governing information flow within specific contexts (e.g., medical data is governed by different norms than banking data).
* **Daniel Solove’s Taxonomy of Privacy:** Focuses on specific activities that violate privacy, such as surveillance, interrogation, aggregation, and secondary use.
* **Ryan Calo’s Harm Dimensions:** Distinguishes between *objective harms* (measurable, observable violations) and *subjective harms* (the perception of unwanted observation or expectation of harm).

### 1.3 What Are Privacy Risks?
Privacy risk is defined as the likelihood that a threat will exploit a vulnerability combined with the resulting impact.
* **Threat Agents:** Can be internal (malicious insiders, careless employees) or external (hackers).
* **Common Threats:** Identity theft, lost or stolen unencrypted devices, and social engineering attacks like "phishing" or "spear-phishing."
* **Impact:** Costs include financial loss (e.g., identity fraud costs), reputation damage, system downtime, and loss of customer trust.

### 1.4 Security, Privacy, and Data Governance
The chapter clarifies that while security supports privacy, they are not equivalent.
* **Security:** Traditionally focuses on the "CIA triad": Confidentiality, Integrity, and Availability.
* **Privacy:** Broader than security. It involves individual autonomy, control over the granularity of information, and freedom from intrusion (e.g., surveillance or exposure).
* **Governance:** Effective privacy requires coordination between IT (CISO), Privacy Officers (CPO), and Legal departments. Data governance policies often stem from regulations like HIPAA, SOX, or GDPR.

### 1.5 Privacy Principles and Standards
IT professionals should use established principles to guide system design. The text details the **OECD Guidelines (1980)** as a foundational standard:
1.  **Collection Limitation:** Data should be obtained lawfully and with consent.
2.  **Data Quality:** Data must be relevant, accurate, complete, and up-to-date.
3.  **Purpose Specification:** Purposes for data collection must be specified at the time of collection.
4.  **Use Limitation:** Data should not be disclosed or used for other purposes without consent or legal authority.
5.  **Security Safeguards:** Reasonable safeguards must be in place against loss, access, or destruction.
6.  **Openness:** Policies regarding data should be transparent.
7.  **Individual Participation:** Individuals have rights to confirm, access, and challenge data held about them.
8.  **Accountability:** Data controllers are accountable for complying with these principles.

### 1.6 The Data Life Cycle
A framework for managing data flow to ensure end-to-end privacy protection:
* **Collection:** Can be active (subject aware) or passive (subject unaware). Includes first-party, third-party, and surveillance collection.
* **Consent:** Mechanisms include "Explicit" (checkboxes, buttons) and "Implicit" (policy links).
* **Processing & Disclosure:** Ensuring data is used only for stated purposes. "Repurposing" data creates significant privacy risks.
* **Retention:** Data should be retained only as long as necessary for business or legal needs.
* **Destruction:** Proper sanitization of media (overwriting, degaussing, incinerating) when data is no longer needed.

The chapter contrasts two organizational cultures regarding this cycle:
* **Maximize Information Utility:** Collects everything, retains indefinitely, and shares broadly to drive innovation.
* **Minimize Privacy Risk:** Collects only what is needed, retains only for the transaction, and strictly limits disclosure.

***

### 2.1.1 The Privacy Engineer

* **Role and Background**
    * **Function:** Serves as both an active member of project teams and a repository of knowledge for interacting with various stakeholders in the software ecosystem.
    * **Typical Career Path:** Often begins as a software developer, advances to a software project manager, and then transitions into the role of a privacy engineer.

* **Key Responsibilities**
    * **Requirements Collection:** Gathers critical regulatory requirements from legal teams to ensure marketing and other project requirements align with laws and social norms.
    * **Requirement Proposal:** Proposes additional privacy-related requirements specific to the project's needs.
    * **Design Collaboration:** Works with designers to discuss best practices and assess potential solutions, including the use of privacy-enhancing technologies (PETs) during the translation of requirements into design specifications.
    * **Verification:** Helps verify that design specifications are correctly implemented and achieve the intended privacy-related effects.
    * **Post-Deployment Support:** Assists with support after deployment, including addressing privacy-related bug fixes and enhancements.
    * **Monitoring and Feedback:** Collects user feedback and monitors external sources (blogs, mailing lists, news) for new privacy incidents to update relevant practices.

* **Community of Practice**
    * **Definition:** The privacy engineer develops a "community of practice," defined as a collective learning process focused on a shared enterprise (e.g., reducing privacy risks in technology).
    * **External Engagement:** This community includes professionals outside the engineer's own organization, such as bloggers, privacy law scholars, and nonprofits.
    * **Sources of Information:**
        * **Scholars & Nonprofits:** Monitoring thought leaders (e.g., Ryan Calo on autonomous vehicles) and organizations (e.g., Center for Democracy and Technology) for policy research on topics like location tracking and identity management.
        * **Regulators:** Tracking guidelines from bodies like the Federal Trade Commission (FTC), including case highlights and workshop summaries, to understand consensus on emerging privacy issues.
        * **Professional Associations:** Participating in workshops and training from organizations like the IAPP to network and improve professional practices.
    * **Goal:** To stay current on how privacy evolves with technology and to provide relevant, updated input into the organization's engineering processes.

* **Ethical Engineering Practice**
    * **Guidance Sources:** Communities of practice serve as a source for ethical guidance.
    * **Ethical Codes:** Privacy engineers rely on codes of ethics from major professional societies, including:
        * Association for Computing Machinery (ACM)
        * Institute of Electrical and Electronics Engineers (IEEE)
        * British Computer Society (BCS)
        * International Council on Systems Engineering (INCOSE)
    * **Evolution of Ethics:** While historical codes focused on professional conduct, modern codes emphasize the ethical obligation to address system privacy and security within complex sociotechnical environments.

### 2.1.3 Software Process Models

* **Origins and Purpose**
    * **Origin:** The concept of software engineering emerged from the 1968 NATO conference (Garmisch report) to manage engineering complexity.
    * **Goal:** To coordinate the construction of software, moving from personal projects to large-scale endeavors requiring sophisticated ecosystems and processes.

* **Core Software Development Activities**
    * Regardless of the specific process model used, six key activities are universally addressed:
        * **Requirements Engineering:** Identifying system constraints, stakeholder goals, and functional/behavioral properties (including privacy).
        * **Design:** Creating architectures that define how the system operates, including modular components and data flows.
        * **Implementation:** Translating design into source code and developing setup/configuration processes.
        * **Testing:** Verifying the system conforms to requirements through test cases and user testing (finding ways to "break" the system).
        * **Deployment:** Installing, configuring, and training users in the operational environment.
        * **Maintenance:** Updating software to fix bugs or add functionality post-deployment.

* **Types of Process Models**
    * **Plan-Driven Methods (e.g., Waterfall, Spiral):**
        * **Characteristics:** Emphasize up-front documentation and planning.
        * **Privacy Integration:** Privacy must be addressed early (CONOPS and requirements phase). Retrofitting privacy later is costly.
        * **Spiral Model:** Focuses on risk analysis. Iterative development where the team reassesses risks (feasibility, design alternatives) at each stage.
    * **Agile Methods (e.g., Extreme Programming, Scrum):**
        * **Characteristics:** Emphasize personal communication and small, dynamic teams.
        * **Scrum:** Uses timeboxing (sprints) and manages requirements via a "product backlog" of user stories.
        * **Privacy Integration:** Privacy engineers participate in developing user stories to identify risks. They review sprint backlogs to catch privacy dependencies (e.g., ensuring parental consent mechanisms are built before collecting child data).

* **DevOps and DevSecOps**
    * **DevOps:**
        * Integrates development and operations to speed up deployment.
        * Visualized as a figure eight; emphasizes continuous feedback and automation.
        * **Challenge:** High velocity can make ensuring privacy and security difficult.
    * **DevSecOps:**
        * Evolution of DevOps that integrates security throughout the lifecycle using automated tools (scanning, patching, testing).
        * **Note:** Privacy (beyond basic confidentiality) is not yet similarly integrated into this model.

* **Privacy Engineering Frameworks**
    * **Holistic Lifecycles:** Methods that act as specialized lifecycles themselves.
        * *OASIS Privacy Management Reference Model and Methodology (PMRM)*
        * *PRIPARE (Privacy and security-by-design)*
    * **Atomic Methods:** Aimed at specific engineering activities.
        * *LINDDUN:* A threat modeling method (discussed further in section 2.2.1.1).
        * *NIST Privacy Risk Assessment Methodology (PRAM):* For risk assessment (discussed further in section 2.2.2.3).
    * **Selection:** The choice of method depends on system complexity, environmental context, and organizational standards.

### 2.1.4 Defect, Fault, Error, Failure, and Harm

* **The Chain of Causality (IEEE Definitions)**
    * **Defect:** A flaw in the requirements, design, or implementation (e.g., coding mistake) that can cause a fault.
    * **Fault:** An incorrect step, process, or data definition within the computer program itself.
    * **Error:** The discrepancy between a computed/observed condition and the correct, specified condition.
    * **Failure:** The system's inability to perform its required function within the specified requirements.
    * **Harm:** The actual or potential negative impact ("hazard") on an individual's privacy.

* **Example Scenario: Unauthorized Disclosure of PII**
    * **Defect:** Source code that fails to correctly check for authorization.
    * **Fault:** The execution of that specific line of flawed code.
    * **Error:** The internal system state where unauthorized access is granted (deviating from the correct state of "access denied").
    * **Failure:** The external event where sensitive PII is actually disclosed to a third party.

* **Types of Privacy Harm**
    * **Objective Harm:** The unanticipated or coerced use of information against a person (measurable/observable).
    * **Subjective Harm:** The perception of unwanted observation (fear/discomfort), regardless of whether it actually occurs.

* **Daniel Solove’s Taxonomy of Privacy Problems**
    Solove categorizes privacy harms into four distinct risk categories:
    1.  **Information Collection:** (e.g., surveillance, interrogation).
    2.  **Information Processing:** (e.g., aggregation, identification, secondary use).
    3.  **Information Dissemination:** (e.g., breach of confidentiality, exposure).
    4.  **Invasion:** (e.g., intrusion, decisional interference).

* **The Engineer's Role**
    * Public reports usually focus on the **Failure** or **Harm** (the outcome).
    * Engineers must investigate to find the underlying **Errors, Faults, and Defects** (the root causes).
    * Engineers can use regulatory enforcement actions and news reports to identify these underlying technical causes and prevent similar privacy violations in their own systems.

### 2.1.5 Relevant Frameworks

* **Purpose and Usage**
    * Frameworks act as a collective resource for privacy engineers, covering requirements, design, controls, skills, and ethics.
    * Organizations typically leverage multiple frameworks rather than relying on just one, as different frameworks align with different aspects of practice.

* **Requirement and Control Frameworks**
    * **Generally Accepted Privacy Principles (GAPP):** Developed by CPA Canada and the AICPA. It serves as a source of general privacy requirements and includes a maturity model for detailed criteria.
    * **NIST Privacy Framework:** Breakdowns privacy risk management into five core activities: Identify, Govern, Control, Communicate, and Protect. These categories function as high-level requirements.
    * **NIST Special Publication 800-53 (Revision 5):** A comprehensive catalog of security and privacy controls. While intended for U.S. federal systems, it is widely used by non-government organizations for its detail and assessment guidance.
    * **ISO/IEC 27701:** An extension of the ISO/IEC 27001 security standard. It details controls specifically for Privacy Information Management Systems (PIMS).
    * **ISO/IEC 27550:** Addresses the relationship between privacy engineering and other engineering disciplines (like systems and security engineering) and common life cycle processes.

* **Privacy by Design (PbD) Standards**
    * **ISO 31700:** A standard addressing Privacy by Design for consumer goods and services, containing 30 high-level requirements.
    * **Institute of Operational Privacy Design (IOPD) Design Process Standard:** Specifies process requirements for integrating privacy into products and services across the entire life cycle.

* **Workforce and Skills**
    * **NIST NICE Framework (SP 800-181):** Provides a standard structure for defining cybersecurity and privacy roles and the necessary competencies.
    * **Privacy Workforce Taxonomy:** An initiative by NIST to align workforce definitions with the NIST Privacy Framework.

* **Ethical Frameworks**
    * **Professional Codes:** Ethical codes from societies like the ACM serve as frameworks for ethical engineering practice.
    * **ACM Code of Ethics:** Includes principles with direct implications for privacy engineering, such as "Respect Privacy" (Principle 1.6) and "Avoid Harm" (Principle 1.2), which help inform design trade-offs.
### 2.2 Privacy Risk Management

* **Overview**
    * Risk management is integral to developing reliable systems.
    * IT development involves **programmatic risk** (cost/schedule) and **technical risk** (technology specifics).
    * IT professionals often perform the role of a **privacy risk analyst**.

* **Definition of Risk**
    * **Classic Equation:** Risk = (Probability of an adverse event) × (Impact of the event).
    * **Numerical Measurement:**
        * Uses specific numbers for probability and impact to compute a comparable score.
        * **Challenge:** Often lacks the necessary empirical data or technical basis to be accurate.
    * **Ordinal Measurement:**
        * Uses relative scales (e.g., Low, Medium, High).
        * **Limitations:** Subjective; relies on human perception/bias; cannot be used for standard arithmetic (e.g., Low + High ≠ Medium).
    * **Quantitative Argument:** Authors like Hubbard, Seiersen, Freund, and Jones argue that quantitative measures are ascertainable and preferable to the imprecision of qualitative/ordinal measures.

* **Privacy Risk Scale (Bhatia and Breaux)**
    * An empirically validated scale based on the theory of perceived risk (Slovic).
    * **Method:** Surveys data subjects on their willingness to share data based on social/physical distance to a privacy harm.
    * **Scoring:** Measurements are multiplicative (e.g., a score of 4 is twice as risky as 2).
    * **Findings:**
        * "Induced disclosure" is perceived as lower risk than "surveillance" and "insecurity."
        * Higher perceived benefits lead to lower perceived risk (consistent with the Privacy Paradox).
        * Likelihood was not found to be a multiplicative factor in computing privacy risk for data subjects.

* **Risk Model Components**
    * Effective management requires a risk model that aligns three elements:
        1.  **Threats**
        2.  **Vulnerabilities** (exploited by threats)
        3.  **Risks** (Adverse Events with likelihood and impact)

* **Risk Management Framework**
    * A step-by-step process for applying the risk model to identify and manage risks.

* **Risk Management Options**
    * **Accept:** Acknowledge the risk but take no action (suitable for low risks or when fixing is too costly).
    * **Transfer:** Shift responsibility to another entity (e.g., purchasing cyber insurance, using third-party vendors).
    * **Mitigate:** Implement controls or design changes to reduce likelihood or impact (e.g., using encryption to mitigate the risk of device theft).
    * **Avoid:** Stop the activity, data collection, or system functionality that causes the risk. (Note: This is often difficult or impossible for essential functions).
    * **Risk-Risk Trade-off:** A situation where a decision to manage one risk inadvertently introduces a new, different risk.

### 2.2.1 Privacy Risk Modeling

* **Purpose**
    * To construct a comprehensive model that addresses **Threats**, **Vulnerabilities**, and **Consequences** (Adverse Events).
    * Each component (Threat, Vulnerability, Consequence) constitutes a model in itself.

* **Modeling Approaches**
    * **System-Specific:** Developing a custom model from scratch tailored to the specific system.
    * **Preexisting Models:** Using established, off-the-shelf models (e.g., LINDDUN, Solove’s Taxonomy).
    * **Hybrid:** Using a preexisting model as a baseline and tailoring it to fit the specific system context.

* **Execution Strategy**
    * Analysts frequently start with a **Consequence Model** (Adverse Events) because it connects directly to impact.
    * From the consequence, analysts work backward to identify the relevant vulnerabilities and threats.
    * **Warning:** Relying solely on consequence models can lead to an abbreviated analysis that misses the root causes (threats/vulnerabilities).

* **Available Component Models**
    * **Consequence Models:** Compliance Model, Fair Information Practice Principles (FIPPs), Subjective/Objective Dichotomy (Calo), Taxonomy of Privacy Problems (Solove), NIST Problems for Individuals.
    * **Vulnerability Models:** Contextual Integrity (Nissenbaum), NIST Problematic Data Actions.
    * **Threat Models:** LINDDUN, MITRE PANOPTIC™.

### **LINDDUN**

**LINDDUN** is a privacy threat modeling framework developed by the DistriNet Research Unit at KU Leuven (Belgium). It is designed to help privacy engineers and developers identify and mitigate privacy threats in software architectures during the early design phases. It is analogous to the STRIDE framework used for security threat modeling but focuses specifically on privacy.

  * **Official Website:** [linddun.org](https://linddun.org/)
  * **Core Methodology:** Data Flow Diagram (DFD) analysis.

#### **The Acronym (Threat Categories)**

The framework is named after the seven privacy threat categories it identifies:

1.  **L - Linkability:** The ability to link two or more items of interest (e.g., packets, records, actions) to the same user, even if their identity is unknown.
2.  **I - Identifiability:** The ability to identify a data subject from a set of data items.
3.  **N - Non-repudiation:** The inability of a data subject to deny a claim (e.g., having performed an action or sent a message), which is often a security goal but a privacy threat (e.g., for whistleblowers).
4.  **D - Detectability:** The ability to distinguish whether an item of interest exists or not (e.g., determining if a person is in a sensitive database).
5.  **D - Disclosure of Information:** The exposure of information to entities who are not authorized to access it.
6.  **U - Unawareness:** The data subject is unaware of the collection, processing, storage, or sharing of their personal data (often related to lack of notice/consent).
7.  **N - Non-compliance:** The processing of data is not compliant with legislation, regulations, or internal policies.

#### **Methodology Variants**

LINDDUN offers different levels of depth to suit different teams:

  * **LINDDUN GO:** A lightweight, card-based brainstorming method designed for agile teams and quick assessments.
  * **LINDDUN PRO:** A rigorous, systematic approach that involves creating a Data Flow Diagram (DFD), mapping threats to every element (entities, processes, data stores, data flows), and documenting everything thoroughly.
  * **LINDDUN MAESTRO:** An advanced, model-based approach (currently in development/research) for automated analysis.

-----

### **MITRE PANOPTIC™**

**MITRE PANOPTIC™** (Pattern and Action Nomenclature Of Privacy Threats In Context) is a newer privacy threat taxonomy developed by MITRE. Unlike LINDDUN, which is model-centric (analyzing a system diagram), PANOPTIC is data-driven and empirically derived from analyzing hundreds of real-world privacy incidents. It is designed to be the privacy equivalent of **MITRE ATT\&CK®**, focusing on the "tactics and techniques" of privacy violations.

  * **Primary Resource:** [suspicious link removed]
  * **Reference Paper:** [USENIX SOUPS Poster/Abstract](https://www.usenix.org/conference/soups2024/presentation/katcher-poster)

#### **Core Components**

PANOPTIC creates a structured vocabulary for privacy threats by splitting them into two taxonomies: **Contextual Domains** (where/who) and **Privacy Activities** (what happened).

**1. Contextual Domains**
These describe the environment and nature of the data involved:

  * **Environment:** Where the event occurs (e.g., Digital, Physical).
  * **Distribution:** The flow of data (e.g., One-to-One, One-to-Many).
  * **Interaction:** The relationship between the user and the entity (e.g., User-initiated, Third-party observation).
  * **Engagement:** Involvement of specific populations (e.g., Vulnerable populations, Children).
  * **Data Type:** The specific category of data (e.g., Biometric, Financial, Location).

**2. Privacy Activities**
These describe the specific actions (or inactions) that constitute the threat:

  * **Notice & Consent:** Issues with transparency (e.g., Misleading notice, Conditioning access on consent).
  * **Collection:** How data is gathered (e.g., Surveillance, Interrogation).
  * **Identification:** Linking data to individuals (e.g., Fingerprinting, Re-identification).
  * **Manageability:** Issues with user control (e.g., Inability to delete, Inability to correct).
  * **Sharing:** Transferring data to others (e.g., Sale of data, Unintended exposure).
  * **Retention:** Keeping data longer than necessary.
  * **Deviations:** Using data for purposes other than originally stated (secondary use).

-----

### **Contextual Integrity**

**Contextual Integrity (CI)** is a conceptual framework for understanding privacy developed by **Helen Nissenbaum**. Unlike models that define privacy simply as secrecy or control, CI defines privacy as maintaining personal information in alignment with the **informational norms** of a specific social context.

* **Core Concept:** Privacy problems arise not just when data is exposed, but when the flow of information disrupts established social norms.
* **Primary Resource:** [Privacy in Context (Stanford Law Books)](https://www.sup.org/books/title/?id=8862)

#### **The Framework Components**
CI breaks down privacy analysis into specific "contexts" and the "norms" that govern them.

**1. Contexts**
These are socially constructed settings characterized by their own distinct values, goals, and rules.
* **Examples:** Healthcare, banking, education, family, politics.
* **Rule of Thumb:** Information appropriate in one context (e.g., health data in a hospital) may be inappropriate in another (e.g., health data in a workplace).

**2. Informational Norms**
These are the rules governing the flow of information. A norm is defined by five specific parameters:
* **Actors:**
    * **Sender:** Who is sending the information?
    * **Recipient:** Who is receiving it?
    * **Subject:** Who is the information about?
* **Attributes:** The specific type of information (e.g., medical status, salary, GPS location).
* **Transmission Principles:** The constraints or conditions under which information flows (e.g., with consent, under confidentiality, required by law, reciprocal).

#### **How to Use It (The Heuristic)**
The framework provides a method for identifying privacy vulnerabilities in a system:

1.  **Identify the Context:** Determine the social setting (e.g., Medical treatment).
2.  **Establish the Norms:** Define the expected flow based on the five parameters (e.g., Patient $\rightarrow$ Doctor, regarding Symptoms, for Treatment).
3.  **Detect Disruptions:** Analyze if the IT system changes any of the parameters (e.g., changing the recipient to a marketing firm).
4.  **Result:** Any system feature that disrupts these established norms is flagged as a **Privacy Vulnerability**.

#### **Example Scenario**
* **Established Norm:** A patient shares a medical condition with a doctor for the purpose of diagnosis.
* **Disruption:** The system automatically forwards that condition to a pharmaceutical company to send ads to the patient’s home.
* **Analysis:** The norm was disrupted because the **Recipient** changed (Doctor $\rightarrow$ Pharma Company) and the **Transmission Principle** changed (Confidentiality $\rightarrow$ Marketing/Commercial). This is a privacy violation.

### **NIST Problematic Data Actions**

This model is part of the **NIST Privacy Risk Assessment Methodology (PRAM)**. It provides a comprehensive framework for identifying privacy vulnerabilities by focusing on specific data operations.

* **Core Concept:** Instead of viewing vulnerabilities just as flaws or bugs, NIST defines them as **"Problematic Data Actions."**
* **Definition:** These are system behaviors or operations that—even if authorized—create the *potential* for an adverse event (a privacy problem for an individual).

#### **Catalog of Problematic Data Actions**
The NIST model provides a catalog of specific actions that should be analyzed as potential sources of risk:

* **Appropriation:**
    * Using personal information in ways that go beyond what the individual expects or has authorized.
    * *Example:* Using data collected for service improvement to train a public AI model without consent.

* **Distortion:**
    * The dissemination of inaccurate, misleading, or incomplete personal information.
    * *Example:* A credit report containing errors that lower a person's score.

* **Induced Disclosure:**
    * Pressuring individuals to provide personal information (often more than necessary).
    * *Example:* Requiring a phone number to download a whitepaper when an email would suffice.

* **Insecurity:**
    * Lapses in data security safeguards that result in the loss of confidentiality, integrity, or availability.
    * *Example:* Storing passwords in plain text or failing to patch a known server vulnerability.

* **Surveillance:**
    * Tracking or monitoring an individual’s activities, communications, or behaviors that is disproportionate to the system's stated objectives.
    * *Example:* A flashlight app that tracks the user's GPS location.

* **Unanticipated Revelation:**
    * The unexpected exposure of facets of an individual or their data, often resulting from processing or aggregation.
    * *Example:* Revealing a user's political affiliation based on their seemingly neutral browsing history.

* **Unwarranted Restriction:**
    * Imposing unjustified constraints on an individual’s ability to access the system or their own information.
    * *Example:* Failing to provide a "forgot password" feature or refusing to let a user download their own data.

### **Fair Information Practice Principles (FIPPs)**

This model uses established privacy principles as a framework for identifying risks. The FIPPs often serve as the foundation for privacy laws and policy regimes, making this model closely related to the **Compliance Model**.

* **Core Concept:** FIPPs prescribe (require) or proscribe (forbid) specific qualities and behaviors for systems that handle personal information.
* **Adoption:**
    * **Federal Trade Commission (FTC):** Recommends the FIPPs to private industry as a guide for privacy risk management.
    * **U.S. Department of Homeland Security (DHS):** Applies FIPPs to its own internal government practices, specifically within privacy impact assessments.

#### **How to Use It**
Using FIPPs as a risk model involves treating the principles as requirements and identifying where the system might fail to meet them.

* **Interpretation:** Because FIPPs are high-level abstractions (unlike specific legal strictures) and are often relative to the "purpose of the system," developers and analysts must interpret how each principle applies to their specific context.
* **Analysis:** The risk analysis process entails examining system elements to identify threats and vulnerabilities that relate to specific FIPPs.
* **Risk Definition:** A risk is defined as the failure to uphold a specific principle (e.g., a failure to limit use, a failure to provide notice).

### **Subjective/Objective Dichotomy**

This model, proposed by Ryan Calo, categorizes privacy harms based on how they are experienced by the individual. It serves as a coarse-grained reference point for aligning threats and vulnerabilities.

* **Core Concept:** Privacy harms fall into two distinct categories: subjective and objective.
* **Analogy:** Calo compares this to the legal distinction between assault (the threat of unwanted contact) and battery (the actual unwanted contact).

#### **1. Subjective Harms**
These harms are grounded in the individual's internal experience or state of mind.
* **Definition:** The *perception* of unwanted observation, regardless of whether actual observation is taking place.
* **Impact:** This creates fear, discomfort, or anxiety.
* **Consequence:** It can lead to a "chilling effect" where an individual alters their behavior, limits their freedom of expression, or avoids using a system due to the fear of being watched (as noted by Julie Cohen and Alan Westin).
* **Risk Alignment:** Any privacy threat that is *perceivable* by an individual can correspond to a subjective privacy harm.

#### **2. Objective Harms**
These harms are external actions that have measurable, adverse consequences.
* **Definition:** The *unanticipated* or *coerced* use of information concerning a person against that person.
* **Nature:** These are actual events where data is used in a way that negatively impacts the individual (e.g., identity theft, financial loss, reputation damage), distinct from the fear of such events.

#### **How to Use It**
Analysts use this model to assess system elements based on user perception versus actual data usage.
* **Expectation vs. Reality:** Examine elements related to individuals' expectations of how data will be used versus how it is actually used.
* **Consent:** Analyze elements related to surveillance, tracking, and whether the user has consented to the collection and use of their information.

### **Taxonomy of Privacy Problems (Daniel Solove)**

This model classifies privacy risks not by high-level principles, but by identifying 16 distinct activities that create privacy problems. These problems are derived from a cultural analysis of law, history, and philosophy. They are organized into four categories.

#### **1. Information Collection**
Harms caused by the gathering of data.
* **Surveillance:** The observation or capturing of an individual’s activities.
    * *Example:* An advertising site embedding hidden frames to track users across the web.
* **Interrogation:** Actively questioning or probing an individual for information.
    * *Example:* A website requiring a mobile phone number for registration when it is not functionally necessary.

#### **2. Information Processing**
Harms caused by the storage, analysis, and manipulation of data.
* **Aggregation:** Combining multiple pieces of information to create a profile that is greater than the sum of its parts.
    * *Example:* Inferring a customer is pregnant by correlating purchases of unscented lotion and vitamins.
* **Identification:** Linking information to a specific individual.
    * *Example:* Using cookies or device fingerprints to link browsing history to a real identity.
* **Insecurity:** Failures to properly protect information from leaks or improper access.
    * *Example:* Failing to encrypt private communications.
* **Secondary Use:** Using information without consent for a purpose different from the one for which it was collected.
    * *Example:* Using an email address collected for shipping updates to send marketing emails.
* **Exclusion:** Denying an individual knowledge of, or participation in, the processing of their data.
    * *Example:* A data broker selling a consumer's profile without the consumer knowing the profile exists.

#### **3. Information Dissemination**
Harms caused by revealing data to others.
* **Breach of Confidentiality:** Revealing information that was promised to be kept secret.
    * *Example:* A platform sharing user data with a third party despite a privacy policy promising not to.
* **Disclosure:** Revealing truthful information that negatively affects how others view the individual.
    * *Example:* Exposing a list of members of a private lifestyle service.
* **Distortion:** Spreading false or inaccurate information about a person.
    * *Example:* A background check service incorrectly labeling an applicant as a felon.
* **Exposure:** Revealing information that is normally concealed (often physical/bodily details) or embarrassing.
    * *Example:* Broadcasting a user's purchase of sensitive health products to their social network.
* **Increased Accessibility:** Making information easier to obtain than it was previously.
    * *Example:* Taking public records that were hard to access physically and putting them online where they are searchable.
* **Blackmail:** Threatening to disclose information to coerce someone.
    * *Example:* Threatening to release medical data unless a ransom is paid.
* **Appropriation:** Using someone’s identity or likeness for another's benefit.
    * *Example:* Using a user's profile photo in an advertisement without their permission.

#### **4. Intrusion and Decisional Interference**
Harms caused by invading an individual’s private life or autonomy.
* **Intrusion:** Acts that disturb an individual’s solitude or tranquility.
    * *Example:* Sending push notifications to a user's phone when they walk past a store.
* **Decisional Interference:** Inserting oneself into an individual's decision-making process regarding their personal affairs.
    * *Example:* Hiding negative reviews to manipulate a user into buying a specific product.

### **NIST Problems for Individuals**

This model is the counterpart to the **NIST Problematic Data Actions** (vulnerabilities). It catalogs the specific adverse events or impacts that individuals may suffer as a result of those data actions.

* **Core Concept:** These are the negative outcomes ("problems") that constitute privacy risk when they result from system operations.
* **Relationship:** There is not necessarily a one-to-one relationship; a single problematic data action can result in multiple problems for individuals.

#### **Catalog of Problems**
The NIST framework identifies eight distinct categories of problems:

* **Loss of Autonomy:**
    * Involves self-imposed restrictions on behavior, expression, or movement (often called the "chilling effect") due to the fear of observation or judgment.

* **Exclusion:**
    * Denying individuals knowledge about their personal information or the ability to act upon that knowledge (e.g., inability to access or correct data).

* **Loss of Liberty:**
    * Improperly raising the possibility of arrest, detainment, or imprisonment.

* **Physical Harm:**
    * Involves direct bodily harm or injury to an individual.

* **Stigmatization:**
    * Linking information to an identity in a way that stigmatizes the person associated with that identity (e.g., public embarrassment, damage to reputation).

* **Power Imbalance:**
    * Enabling abusive, unfair, or coercive treatment of an individual by leveraging information asymmetries.

* **Loss of Trust:**
    * The erosion of confidence resulting from violations of implicit or explicit expectations or agreements regarding the treatment of personal information.

* **Economic Loss:**
    * Involves direct financial loss (e.g., theft) or indirect financial loss (e.g., higher prices, lost job opportunities).

Based on section 2.2.2 of the text, here is a detailed summary of the **Privacy Risk Management Framework**.

### **2.2.2 Privacy Risk Management Framework**

* **Overview**
    * **Purpose:** Provides a step-by-step process for applying a risk model to a specific information system to identify and manage risks.
    * **vs. Risk Models:** While risk models (like LINDDUN or Solove's) address domain-specific issues (what the risks *are*), frameworks address the *process* (how to find and fix them).
    * **Generic Frameworks:** ISO 31000 is a generic risk management framework that can be adapted for privacy.
    * **The Six Steps:**
        1.  Characterization
        2.  Threat, Vulnerability, and Event Identification
        3.  Risk Assessment
        4.  Risk Response Determination
        5.  Risk Control Implementation
        6.  Monitoring and Reviewing

#### **Step 1: Characterization (2.2.2.1)**
* **Goal:** To describe the system in a way that makes it amenable to privacy risk analysis.
* **Key Activities:**
    * Identify the system's purpose.
    * Map how personal information flows through the system (using the data life cycle).
    * Identify supporting technologies.
* **Tools:**
    * **Use Cases:** Descriptions of system behavior (e.g., "Personalized customer promotions").
    * **Data Flow Diagrams:** Visualizing the movement of data.
* **Context:** The amount of context required depends on the chosen risk model (e.g., Contextual Integrity requires identifying social norms and stakeholders).

#### **Step 2: Threat, Vulnerability, and Event Identification (2.2.2.2)**
* **Goal:** To identify the specific components of the risk model (Threats, Vulnerabilities, Adverse Events).
* **Methodology:**
    * **Identify the Anchor Point:** Start with the element emphasized by your chosen risk model.
        * *Solove/Calo:* Start with **Adverse Events** (what can go wrong?).
        * *Contextual Integrity:* Start with **Vulnerabilities** (what norms are disrupted?).
    * **Work Backward/Forward:** Once one element is identified, determine the others.
        * *Example:* If the Event is "Disclosure of purchase history," work backward to find the Threat (e.g., "Law enforcement request").

#### **Step 3: Risk Assessment (2.2.2.3)**
* **Goal:** Assign **Likelihood** and **Impact** scores to the identified risks to prioritize them.
* **Scoring:**
    * **Likelihood:** Probability of the vulnerability being exploited (Low/Medium/High or 0.0–1.0).
    * **Impact:** The magnitude of the adverse event. Often subjective (ordinal values) rather than purely financial.
* **Specific Assessment Methodologies:**
    * **NIST Privacy Risk Assessment Methodology (PRAM):**
        * Uses worksheets to assess business objectives and system design.
        * Prioritizes risk by scoring "Problematic Data Actions" (vulnerabilities) and "Problems for Individuals" (impacts) on a 10-point scale.
    * **FAIR-P (Factor Analysis of Information Risk for Privacy):**
        * Focuses on quantitative, probabilistic estimates.
        * Breaks risk into **Frequency** (Likelihood) and **Magnitude** (Impact).
        * Focuses specifically on risks to *individuals* rather than the organization.
    * **STPA-Priv (System-Theoretic Process Analysis for Privacy):**
        * Qualitative approach based on control theory.
        * identifies risks as **erroneous control actions** (e.g., a control not applied, applied too late, or applied incorrectly).

#### **Step 4: Risk Response Determination (2.2.2.4)**
* **Goal:** Decide what to do about the assessed risks based on constraints (time, money, personnel).
* **Response Options:**
    * **Accept:** Acknowledge the risk but take no action (suitable for low risks).
    * **Transfer:** Shift responsibility (e.g., using a compliant third-party payment processor).
    * **Mitigate:** Implement controls to reduce likelihood or impact (e.g., adding encryption).
    * **Avoid:** Change the system design to eliminate the risk entirely.
        * *Example:* Avoiding an "Interrogation" risk by replacing a "Birth Date" field with a simple "Are you over 18?" checkbox.

#### **Step 5: Risk Control Implementation (2.2.2.5)**
* **Goal:** Implement specific measures to execute the chosen risk response.
* **Control Categories:**
    * **Administrative:** Policies, training, governance procedures (e.g., appointing a privacy officer).
    * **Technical:** System-level safeguards (e.g., access controls, encryption, consent mechanisms).
    * **Physical:** Locks, secure storage for hard copies.
* **NIST SP 800-53:** A primary catalog for selecting specific technical and administrative controls.

#### **Step 6: Monitor and Review (2.2.2.6)**
* **Goal:** Ensure controls remain effective as the system and environment evolve.
* **Triggers:** Establish automatic triggers for re-assessment, such as:
    * Modifying code.
    * Adding new database tables.
    * Configuring new services.
* **Auditing:** Continuously testing controls (e.g., checking if personnel training is effective, reviewing complaint resolution logs).
